{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0c31dd-55c9-4e48-9ea5-b905689fd0d6",
   "metadata": {},
   "source": [
    "## Running a HuggingFace model locally\n",
    "\n",
    "**You will need to clone this GitHub repository or download this Jupyter Notebook to run locally.**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befcb104-28dc-4088-bbbc-93d271b76879",
   "metadata": {},
   "source": [
    "In this notebook we are going to learn how to run a Hugging Face model locally. If you don't have a GPU in your local machine (laptop), that is still fine; you can run the final piece which shows a llama.cpp implementation. Please refer to the README file for more information regarding the python installations and llama.cpp.\n",
    "\n",
    "Pros of HuggingFace Transformers library:\n",
    "* downloads the models automatically\n",
    "* there are snippets of code available to run any model\n",
    "* easily intergratable models to your project\n",
    "\n",
    "Cons:\n",
    "* you need to code the behaviour / user interaction\n",
    "* it's not as fast as other alternatives and fails to run moderate sized LLMs (>3-4B parameters)\n",
    "* large computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064bbc8",
   "metadata": {},
   "source": [
    "#### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision==0.17.2\n",
    "!pip install -U torch==2.3.1\n",
    "!pip install tensorflow==2.18.0\n",
    "!pip install transformers==4.47.0\n",
    "!pip install datasets==3.2.0\n",
    "!pip install ipywidgets==8.1.5\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598294a-8613-4ed7-bbc4-ffab196546dc",
   "metadata": {},
   "source": [
    "#### Importing packages and checking GPU access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9e667d-e3c2-4576-8c26-6d33edc13bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic packages\n",
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697ccc8c-66a5-425f-b245-feec26557a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47.0\n"
     ]
    }
   ],
   "source": [
    "# make sure you have an updated transformers version (>4.43)\n",
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7acc82c-3704-4018-855b-09c17afff5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both should return 'True' if running on MacOs\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "# Should recognize 1 GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c395060-ac48-45f4-a467-e0722f77510e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable the GPU\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bcd85-8ca9-4ca5-a56e-491c6905a3e5",
   "metadata": {},
   "source": [
    "#### Authenticate through HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b608eeb-fb76-4e34-8af4-372cf3c3ba14",
   "metadata": {},
   "source": [
    "HuggingFace requires to login and use a specialised token to use most of their models and datasets. We suggest creating a HuggingFace account and going to settings to create an access token. Use a name of your prefrerence and create either a read or a write token. Useful video walkthrough on how to generate tokens:https://www.youtube.com/watch?v=Br7AcznvzSA . Try to not share your token when sharing this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cc8bb-b7d4-49bf-b6bb-fcde42927213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace with your token\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# Alternatively, you can login via terminal with this command: huggingface-cli login \n",
    "# Add your token and hit Y when prompted to add token as git credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8297a0e-57ec-40f8-819f-454b26eb3a06",
   "metadata": {},
   "source": [
    "#### Load and run HuggingFace models (non-LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c9f584-dbc6-41e2-82d3-4d4514893e0e",
   "metadata": {},
   "source": [
    "You need to find a model you'd like to use in HuggingFace and click on the right top corner to use it with HuggingFace. Copy and paste that code block here. Make sure you include a pipeline, an input_text and an output argument to call the model. Some useful examples are provided below to showcase this.\n",
    "\n",
    "The model name or model_id can be copied from the top part of the page, it will be somethng like 'meta-llama/Llama-2-7b-chat-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d34492b-3a2c-4b1b-9648-f16421fc530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache() # This frees up unused memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3aec37-24bc-457c-a778-86cfd052f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of HuggingFace models for different NLP tasks in Investment Analysis\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "DEVICE = 0  # Use 0 for GPU or -1 for CPU\n",
    "\n",
    "## Sentiment Analysis for Financial News\n",
    "# Analyze the sentiment of market news or analyst reports\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "news = \"The stock market rallied today after the Federal Reserve's announcement.\"\n",
    "results = classifier(news)\n",
    "print(f\"Sentiment: {results[0]['label']} with score {results[0]['score']}\")\n",
    "\n",
    "## Named Entity Recognition (NER) for Investment Documents\n",
    "# Extract named entities such as companies, locations, and financial entities\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=DEVICE)\n",
    "\n",
    "financial_doc = \"Tesla reported strong earnings this quarter, with revenue reaching $21 billion.\"\n",
    "ner_results = nlp(financial_doc)\n",
    "print(\"Named Entities:\")\n",
    "for entity in ner_results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c70f56-558e-4877-82b4-4b3dbfd4305d",
   "metadata": {},
   "source": [
    "#### Load and run HuggingFace models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41f34c-f4b5-45cb-b54b-98e5cb1dcc73",
   "metadata": {},
   "source": [
    "We are going to see some examples of running LLMs using HuggingFace and Langchain below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735ebd87-ee5e-4642-9082-c2e45888c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache() # This frees up unused memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7f142-2467-43d0-8df7-5e3732fd2601",
   "metadata": {},
   "source": [
    "Example 1: CAUTION! The following is an example that will run OOM (out-of-memory) if run on a ~18GB GPU. We do not advise to run it as it will probably crash your machine applications, that's why we have left it commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a8a5975-2b87-4afa-ba61-c60cb0286a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE of HuggingFace Mistral-7B-v0.1 \n",
    "# with torch.no_grad():\n",
    "\n",
    "# # Create a pipeline for text generation or instruction-following    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "#     pipe = pipeline(\"text-generation\", model = model, tokenizer = tokenizer, device = DEVICE)     # Load model directly\n",
    "\n",
    "#     # Provide a financial instruction or a prompt\n",
    "#     input_text = \"Analyze the impact of rising interest rates on global stock markets.\"\n",
    "    \n",
    "#     # Generate the response from the model\n",
    "#     output = pipe(input_text, max_length=150, truncation=True)\n",
    "    \n",
    "#     # Print the result\n",
    "#     print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685e58b-605a-49a1-ba7c-64caec8accdf",
   "metadata": {},
   "source": [
    "Example 2: In the below snippet you will find a sample run of a relatively small LLM. To run an LLM of your choice you need to adapt the code for the tokenizer and model. Also, you need to adjust the\n",
    "pipeline to the task. Some key tasks:\n",
    "* \"text-generation\"\n",
    "* \"text-classification\"\n",
    "* \"summarization\"\n",
    "* \"question-answering\"\n",
    "* \"sentiment-analysis\" and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ba6e2-c6bf-4b86-a679-3566a35673c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running an LLM with Langchain\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# Load the model for financial text generation\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"Qwen/Qwen2.5-1.5B-Instruct\", task=\"text-generation\", pipeline_kwargs={\"max_new_tokens\": 200, \"pad_token_id\": 50256},\n",
    ")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a prompt template for investment-related questions\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's analyze this step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create a processing chain\n",
    "chain = prompt | hf\n",
    "\n",
    "# Example investment-related question\n",
    "question = \"What are the key differences between ETFs and mutual funds?\"\n",
    "\n",
    "# Generate and print the response\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883b8233-438d-4f43-8ce8-057931c058bb",
   "metadata": {},
   "source": [
    "#### Saving models locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaccc29-01df-47af-a57e-27f21a73badd",
   "metadata": {},
   "source": [
    "You can use the following commented out section if you want to save models locally and reload. Models take up a lot of space so be mindful of that, or delete when you don't need them anymore on your local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81ebb357-b2b1-489d-8ba5-3cfbc62fb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"TheFinAI/finma-7b-full\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(f\"{model_id}\", legacy = True)\n",
    "# tokenizer.save_pretrained(f\"cache/tokenizer/{model_id}\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(f\"{model_id}\")\n",
    "# model.save_pretrained(f\"cache/model/{model_id}\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(f\"./{model_id}\", legacy=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(f\"./{model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79a2ba-3593-4792-b8e4-5be61501c7bc",
   "metadata": {},
   "source": [
    "#### Note on the LLMs inference behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ba47e-3149-44c6-adf0-df278192a173",
   "metadata": {},
   "source": [
    "You can fine-tune the behaviour of the generation by adjusting parameters such as:\n",
    "\n",
    "* max_length: The maximum length of the generated response in number of tokens.\n",
    "* temperature: Affects randomness in the output (lower values make output more deterministic, while higher values produce more creative responses.\n",
    "* top_k: Limits the sampling pool to the top k most likely tokens at each generation step.\n",
    "* top_p: Filters the sampling pool based on cumulative probability, ensuring only the most probable tokens with a combined probability of p are considered.\n",
    "* num_return_sequences: Specifies the number of generated outputs to return.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99160b05-58b0-478d-9106-cab985695e48",
   "metadata": {},
   "source": [
    "#### Run a model with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4c68d-b38a-46f2-a114-09dc22a58f8a",
   "metadata": {},
   "source": [
    "\n",
    "Let's see some advantages and disadvantages of Langchain:\n",
    "* it uses the HuggingFace Transformer library\n",
    "* it has access to a lot of models\n",
    "* based on a Task-Oriented Framework and not ML-oriented as Transformers\n",
    "* consists of plug-and-play modules\n",
    "\n",
    "Cons:\n",
    "* depends on external LLMs (HuggingFace, OpenAI.. )\n",
    "* not designed for training or fine-tuning models\n",
    "* large computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb87a0a-b291-4fb3-a659-13330f0f6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7616f6-7053-4684-b26e-ca4abbc4858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running an LLM with Langchain\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"microsoft/DialoGPT-medium\", task=\"text-generation\", pipeline_kwargs={\"max_new_tokens\": 200, \"pad_token_id\": 50256},\n",
    ")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "question = \"What are capital markets?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719f365-6f50-4b79-ab9f-883e4f2a9c19",
   "metadata": {},
   "source": [
    "# LLama CPP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34dba1e-739f-4fde-920f-51795d71936d",
   "metadata": {},
   "source": [
    "# Option A: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27ce56",
   "metadata": {},
   "source": [
    "#### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-cpp-python==0.3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec058b-7b90-464f-8f40-9e9eae10506e",
   "metadata": {},
   "source": [
    "Download the model locally, load it and run inference with the following commands. This Option is for models in HuggingFace that have a GGUF format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa93d8-853e-472f-812f-1eaa285ec666",
   "metadata": {},
   "source": [
    "Run the following code command in your MacOs terminal to download a GGUF model of your choice. Adjust the model names accroding to HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b1be9-9db3-4e24-addd-85d1dea4ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to HuggingFace and download the model. Please run the following commands from your terminal.\n",
    "!huggingface-cli login\n",
    "!huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q5_K_S.gguf --local-dir . --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53c133-e6e6-4e5d-8c54-75bdf11b40c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model\n",
    "llm = Llama(model_path='###PATH to YOUR Directory###/Practical Guide For LLMs In the Financial Industry/llama-2-7b-chat.Q5_K_S.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef6dfa-fe3a-4095-85a6-542f5839400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Can you tell me one of the challenges of current capital markets?\"\n",
    "response = llm(sentence, max_tokens=512)\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288035b4-2f81-4a21-82e5-526a3d077570",
   "metadata": {},
   "source": [
    "# Option B: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8491e-666c-491b-8434-0f92f1d1833e",
   "metadata": {},
   "source": [
    "If you are interested in running a model that only has a HuggingFace format (hf) and not a GGUF, you need to convert it first to GGUF and then lead it in memory and perform inference using the llama.cpp package. To do that, you have to clone the llama.cpp repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe404dc0-9b21-436c-8ac6-44d998f494dc",
   "metadata": {},
   "source": [
    "Refer to the README file on how to do that step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09e500-8746-48e6-a208-35aca1a4daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's suppose we're interested in this model_id: meta-llama/Llama-2-7b-chat-hf\n",
    "# Download the model locally using the terminal and the command below\n",
    "# python download.py meta-llama/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c11d0f-a802-40cf-83b6-10160a3fb28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the hf model is downloaded, we need to update the convert-hf-to-gguf-update.py in the llama.cpp folder with \n",
    "# the new model and tokenizer details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ffed5-8f98-4b3a-8a33-2d70fcc028a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we need to run: convert-hf-to-gguf-update.py <huggingface_token>\n",
    "# which will convert the model and save a new model artefact under the gguf file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9613c-a933-4c6c-aa4e-c4891db95b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using that gguf we conduct inference as above, by loading the gguf model\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model\n",
    "llm = Llama(model_path='//your_path_to_model/llama.cpp/llama-2-7b-chat.Q5_K_S.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7e2a3-e9e2-46dc-9def-86fae0553799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Can you tell me one of the challenges of current capital markets?\"\n",
    "response = llm(sentence, max_tokens=512)\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d36cb99-5a0e-4a51-9743-b8623ca049e1",
   "metadata": {},
   "source": [
    "### Load HuggingFace dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44cd78-9369-45d5-9263-8be8cae8e34f",
   "metadata": {},
   "source": [
    "As with the models, you need to browse the datasets and find a suitable for your project. The dataset_id can be copied from the top part of the page, it will be somethng like 'TheFinAI/en-fpb'. You can see how to load and convert a dataset in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb16b93-66c1-4daf-8f78-5cff298c50c0",
   "metadata": {},
   "source": [
    "Load the dataset (e.g., CICM for stock movement prediction)\n",
    "Other datasets:\n",
    "* Sentiment Analysis:\n",
    "* Question-Answering:\n",
    "* Summarisation:\n",
    "* Stock Movement Prediction: 'TheFinAI/flare-sm-cikm', 'TheFinAI/en-fpb', 'TheFinAI/flare-sm-bigdata', 'TheFinAI/flare-ectsum', 'TheFinAI/flare-edtsum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a16588-b379-4738-acd5-b946efaf823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309404c-6468-4bfe-b604-24704cd5df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d54ae5-6eea-46f5-b628-f712793b0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Restart the kernel after running this.\n",
    "# !pip install -U datasets huggingface_hub fsspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615559ce-d518-4e3c-b7ce-0b66404a45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (e.g., IMDB for sentiment analysis)\n",
    "dataset = load_dataset('TheFinAI/flare-sm-cikm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8071f0bc-ac84-4a0c-a535-26ecdcfd68af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'text', 'choices', 'gold'],\n",
      "        num_rows: 3396\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'text', 'choices', 'gold'],\n",
      "        num_rows: 1143\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'text', 'choices', 'gold'],\n",
      "        num_rows: 431\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Display the dataset info and a sample\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b318c04e-3e74-4c1b-a814-62176388f80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>text</th>\n",
       "      <th>choices</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cikmsm3827</td>\n",
       "      <td>Assess the data and tweets to estimate whether...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>date,open,high,low,close,adj-close,inc-5,inc-1...</td>\n",
       "      <td>[Rise, Fall]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cikmsm3828</td>\n",
       "      <td>Analyze the information and social media posts...</td>\n",
       "      <td>Rise</td>\n",
       "      <td>date,open,high,low,close,adj-close,inc-5,inc-1...</td>\n",
       "      <td>[Rise, Fall]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cikmsm3829</td>\n",
       "      <td>Examine the data and tweets to deduce if the c...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>date,open,high,low,close,adj-close,inc-5,inc-1...</td>\n",
       "      <td>[Rise, Fall]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cikmsm3830</td>\n",
       "      <td>Assess the data and tweets to estimate whether...</td>\n",
       "      <td>Rise</td>\n",
       "      <td>date,open,high,low,close,adj-close,inc-5,inc-1...</td>\n",
       "      <td>[Rise, Fall]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cikmsm3831</td>\n",
       "      <td>Assess the data and tweets to estimate whether...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>date,open,high,low,close,adj-close,inc-5,inc-1...</td>\n",
       "      <td>[Rise, Fall]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              query answer  \\\n",
       "0  cikmsm3827  Assess the data and tweets to estimate whether...   Fall   \n",
       "1  cikmsm3828  Analyze the information and social media posts...   Rise   \n",
       "2  cikmsm3829  Examine the data and tweets to deduce if the c...   Fall   \n",
       "3  cikmsm3830  Assess the data and tweets to estimate whether...   Rise   \n",
       "4  cikmsm3831  Assess the data and tweets to estimate whether...   Fall   \n",
       "\n",
       "                                                text       choices  gold  \n",
       "0  date,open,high,low,close,adj-close,inc-5,inc-1...  [Rise, Fall]     1  \n",
       "1  date,open,high,low,close,adj-close,inc-5,inc-1...  [Rise, Fall]     0  \n",
       "2  date,open,high,low,close,adj-close,inc-5,inc-1...  [Rise, Fall]     1  \n",
       "3  date,open,high,low,close,adj-close,inc-5,inc-1...  [Rise, Fall]     0  \n",
       "4  date,open,high,low,close,adj-close,inc-5,inc-1...  [Rise, Fall]     1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset['test'])\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
