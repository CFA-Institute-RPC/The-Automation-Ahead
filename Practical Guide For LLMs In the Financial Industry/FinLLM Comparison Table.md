| **Model**                                                                                                                | **Year** | **Backbone**        | **Open-source (Y/N)** | **CPT or FT Method**                                                                                                                                                                                 | **HuggingFace Link**                                                                                                                                                 | **SA â€“ FPB micro F1**<br>*(FLARE - 5 shots)* | **SA-FiQA weighted F1**<br>*(FLARE - 5 shots)* | **SA-FOMC micro-F1**<br>*(Zero-shot)* | **HC Avg F1**<br>*(FLARE - 5 shots)* | **NER (Entity-level F1)**<br>*(FLARE)* | **QA-FinQA**<br>*(FLARE) (EmAcc)* | **QA-ConvFinQA**<br>*(FLARE) (EmAcc)* | **BigData22**<br>*(FLARE) (Acc/MCC)* | **ACL18**<br>*(FLARE) (Acc/MCC)* | **CIKM18**<br>*(FLARE) (Acc/MCC)* | **ECTSum**<br>*(FLARE) Rouge-1* | **EDTSum**<br>*(FLARE) Rouge-1* |
|---------------------------------------------------------------------------------------------------------------------------|----------|----------------------|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|-----------------------------------------------|-------------------------------------|-------------------------------------|-------------------------------------|------------------------------------|-------------------------------------|-------------------------------------|------------------------------------|-------------------------------------|-------------------------------------|------------------------------------|
| [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT)<br>[Llama2-7B Link](https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_Sentiment_Analysis_v3/benchmark/benchmarks.ipynb) | **2023** | LLaMA 2-7B (base)    | Y                      | LoRA (Instruct fine-tuning) and RLHF. FinGPT is an LLM fine-tuned on financial news, company filings, social media posts, and company announcements.                                                    | [FinGPT on HuggingFace](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)                                                                                    | **0.861**                                   | **0.825**                                    | -                                   | **0.938**                          | **0.678**                          | -                                  | -                                   | **0.45/0.00**                        | **0.49/0.00**                       | **0.42/0.00**                        | Rouge-1: 0                          | Rouge-1: 0                         |
| InvestLM 65B                                                                                                             | **2023** | LLaMA 2-65B          | Y                      | LoRA (Instruct fine-tuning). InvestLM instruction-tunes LLaMA using a curated dataset spanning various financial topics, providing insightful responses to investment-related questions.                | [InvestLM GitHub](https://github.com/AbaciNLP/InvestLM), [InvestLM on HuggingFace](https://huggingface.co/yixuantt/InvestLM2-AWQ)                                  | **0.71**                                    | **0.90**                                     | **0.61**                           | -                                   | -                                   | **0.29**                           | -                                   | -                                    | -                                   | -                                    | **Rouge-1: 0.26**                   | -                                  |
| FinLLaMA                                                                                                                 | **2024** | LLaMA 3 8B           | -                      | Pre-training on financial data using 64 GPU nodes. QLoRA fine-tuning on a 573K sample instruction-tuning dataset tailored for financial applications.                                                   | [FinLLaMA on HuggingFace](https://huggingface.co/TheFinAI/FinLLaMA-instruct)                                                                                       | **0.7025**                                  | **0.7534**                                   | **0.50**                           | **0.8554**                         | **0.8210**                         | -                                  | **0.5141**                          | -                                    | -                                   | -                                    | -                                   | -                                  |
| FinMA 7B Full *(FLARE benchmark)*                                                                                        | **2023** | LLaMA 7B/30B         | Y                      | LoRA (Instruct fine-tuning). FinMA tunes LLaMA using publicly available NLP benchmarks formatted as instructions.                                                                                       | [FinMA on HuggingFace](https://huggingface.co/TheFinAI/finma-7b-full)                                                                                              | **0.87/0.88**<br>(FinBEN)                    | **0.79**                                     | **0.52/0.49**<br>(FinBEN)           | **0.97**                           | **0.69**                           | **0.04**<br>(FinMA paper - FinBEN) | **0.20**                            | **0.51/0.02**                        | **0.56/0.10**                      | **0.53/-0.03**                       | **Rouge-1: 0.08**                   | **Rouge-1: 0.13**                  |
| BloombergGPT                                                                                                             | **2023** | BLOOM 50B            | **N**                  | Training from scratch on financial corpuses.                                                                                                                                                             | N/A                                                                                                                                                                 | **0.5107**<br>(5-shot)                      | **0.7505**<br>(5-shot)                       | -                                   | **0.822**<br>(5-shot)              | **0.61**<br>(20-shot)               | -                                  | **0.43**                            | -                                    | -                                   | -                                    | -                                   | -                                  |
| **Mistral 7B**                                                                                                           | **2023** | -                    | **Y**                  | Trained on generic corpuses.                                                                                                                                                                             | [Mistral 7B on HuggingFace](https://huggingface.co/mistralai/Mistral-7B-v0.1)                                                                                      | **0.29**                                    | **0.16**                                     | **0.37**                           | **0.60**                           | **0.24**                           | **0**                              | **0.31**                            | **0.46/0.02**                        | **0.49/0.00**                      | **0.42/-0.05**                       | Rouge-1: 0                          | Rouge-1: 0.12                      |
| **LLaMA 2 7B Chat**                                                                                                      | **2023** | -                    | Y                      | Trained on generic corpuses.                                                                                                                                                                             | [LLaMA 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat)                                                                                               | **0.39**                                    | **0.76**                                     | **0.35**                           | **0.72**                           | **0.18**                           | **0.00**                           | **0.00**                            | **0.54/0.05**                        | **0.51/0.01**                      | **0.55/-0.03**                       | Rouge-1: 0                          | Rouge-1: 0.17                      |
| **LLaMA 2 70B**                                                                                                          | **2023** | -                    | Y                      | Trained on generic corpuses.                                                                                                                                                                             | [LLaMA 2 70B](https://huggingface.co/meta-llama/Llama-2-70b)                                                                                                       | **0.73**                                    | **0.83**                                     | **0.49**                           | **0.63**                           | **0.04**                           | **0.06**                           | **0.25**                            | **0.47/0.00**                        | **0.51/0.01**                      | **0.49/-0.07**                       | Rouge-1: 0                          | Rouge-1: 0.25                      |
| **LLaMA 3 8B**                                                                                                           | **2024** | -                    | -                      | Trained on generic corpuses.                                                                                                                                                                             | [LLaMA 3.1 8B](https://huggingface.co/meta-llama/Llama-3.1-8B)                                                                                                     | **0.6965**                                  | **0.5229**                                   | **0.41**                           | **0.8059**                         | **0.3918**                         | **0**                              | **0.3195**                          | **0.55/0.02**                        | **0.52/0.02**                      | **0.57/0.03**                        | Rouge-1: 0                          | Rouge-1: 0.14                      |
| **ChatGPT**                                                                                                              | **2023** | -                    | **N**                  | Trained on generic corpuses.                                                                                                                                                                             | -                                                                                                                                                                   | **0.78**                                    | **0.60**                                     | **0.64**                           | **0.77**                           | **0.77**                           | **0.58**                           | **0.60**                            | **0.53/-0.025**                      | **0.50/0.005**                     | **0.55/0.01**                        | Rouge-1: 0                          | Rouge-1: 0.17                      |
| **GPT-4**                                                                                                                | **2023** | -                    | **N**                  | Trained on generic corpuses.                                                                                                                                                                             | -                                                                                                                                                                   | **0.78**                                    | **0.80**                                     | **0.71**                           | **0.86**                           | **0.83**                           | **0.63**                           | **0.76**                            | **0.54/0.03**                        | **0.52/0.02**                      | **0.57/0.02**                        | **Rouge-1: 0.30**                   | Rouge-1: 0.20                      |
| **Gemini**                                                                                                               | **2023** | -                    | **N**                  | Trained on generic corpuses.                                                                                                                                                                             | -                                                                                                                                                                   | **0.77**                                    | **0.81**                                     | **0.40**                           | **0.78**                           | **0.61**                           | **0**                              | **0.43**                            | **0.55/0.04**                        | **0.52/0.04**                      | **0.54/0.02**                        | -                                   | **Rouge-1: 0.39**                  |