{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e56619e4",
      "metadata": {},
      "source": [
        "# Portfolio Material News Updater  \n",
        "\n",
        "Welcome to the **Material News Updater** tutorial. By the end of this notebook you will be able to:\n",
        "\n",
        "1. **Collect** fresh articles from a curated list of financial-news RSS feeds.\n",
        "2. **Embed** those stories in a Chroma vector database with OpenAI embeddings.\n",
        "3. **Query** the database with GPT-4.1 to surface *material* news for a portfolio of stocks.\n",
        "4. **Summarize** the individual stock news briefs into a concise portfolio brief.\n",
        "5. **Email** yourself a concise morning briefing—fully automated.\n",
        "\n",
        "You’ll practise:\n",
        "\n",
        "* Working with RSS feeds.\n",
        "* Turning raw text into embeddings using LangChain + OpenAI.\n",
        "* Building a Retrieval-Augmented Generation (RAG) chain.\n",
        "* Packaging results for convenient distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python ≥ 3.11 (Conda or venv)\n",
        "- OpenAI API key\n",
        "- Gmail credentials stored in a `.env` file (use the `.env.example` template and remove the `.example` suffix)\n",
        "\n",
        "---\n",
        "\n",
        "## OpenAI Account Setup\n",
        "\n",
        "- [Create an OpenAI Platform Account](https://platform.openai.com/signup)\n",
        "- [Generate an API Key](https://platform.openai.com/account/api-keys)\n",
        "\n",
        "---\n",
        "\n",
        "## Gmail Setup (for sending email alerts)\n",
        "\n",
        "This example uses **Gmail** to send the morning brief using `smtplib` with `STARTTLS`. Gmail requires an **App Password** (not your normal password) to be used for sending emails via external scripts.\n",
        "\n",
        "⚠️ **IMPORTANT:** For security and privacy, use a dedicated “junk” Gmail account for this project. Do **not** use your personal or work email.\n",
        "\n",
        "### Step-by-step setup:\n",
        "1. [Create a new Gmail account](https://accounts.google.com/signup)\n",
        "2. Enable **2-Step Verification** on that account:  \n",
        "   https://myaccount.google.com/security\n",
        "3. Generate a **Gmail App Password**:\n",
        "   - Go to your [Google App Passwords page](https://myaccount.google.com/apppasswords)\n",
        "   - Name it something like \"News Brief App\"\n",
        "   - Copy the generated password\n",
        "4. Add it to your `.env` file like this:\n",
        "\n",
        "```env\n",
        "GMAIL_EMAIL=yourjunkemail@gmail.com\n",
        "GMAIL_APP_PASSWORD=your_16_char_password\n",
        "TO_EMAIL=you@example.com\n",
        "```\n",
        "\n",
        "You can also run this notebook via a Google Colab here: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1nEkt2ivAlU2XVU7AkyQG4Q-rMat7UcX8?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "Let’s start by loading the RSS catalogue."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8723b736",
      "metadata": {},
      "source": [
        "## 1. Load RSS sources\n",
        "\n",
        "`news_rss.json` contains a simple list of feed URLs plus some lightweight metadata.  \n",
        "The next cell just deserialises that file into Python so we can iterate over it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e43e0230",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('news_rss.json', 'r') as file:\n",
        "    rss_json = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5694b04f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the below line to install the feedparser package\n",
        "#!pip install feedparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a83c9bbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import feedparser\n",
        "import time\n",
        "\n",
        "feeds = []\n",
        "\n",
        "for rss_dict in rss_json:\n",
        "    rss_url = rss_dict['rss']\n",
        "    source = rss_dict['source']\n",
        "    news_type = rss_dict['type']\n",
        "    feed = feedparser.parse(rss_url)\n",
        "    feed_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n",
        "    feed_dict = {\n",
        "        'source':source,\n",
        "        'type':news_type,\n",
        "        'feed':feed,\n",
        "        'time_pulled':feed_time\n",
        "    }\n",
        "    feeds.append(feed_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "653e4b2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the below to install dot-env\n",
        "#!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f86a463d",
      "metadata": {},
      "source": [
        "## 2. Embed stories and build the vector store\n",
        "  \n",
        "Here we walk through each RSS entry, keep the entire article intact (because they’re short), and prepare a list of `documents` that we’ll feed into Chroma. Each document carries metadata—`title`, `source`, the time we pulled it, and a high-level `news_type` tag—for easier filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8ec97ca7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Create full documents from entries without splitting\n",
        "documents = []\n",
        "for rss_feed in feeds:\n",
        "    try:\n",
        "        for entry in rss_feed['feed']['entries']: \n",
        "            title = entry['title']\n",
        "            entry_content = str(entry)\n",
        "            documents.append({\"content\": entry_content, \"metadata\": {\"title\":title,\"source\": rss_feed[\"source\"], \"time_pulled\": rss_feed[\"time_pulled\"], \"news_type\":rss_feed['type']}})\n",
        "    except Exception as e:\n",
        "        print(f'An error - {e} occured for {entry}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64efc379",
      "metadata": {},
      "source": [
        "### Batch The Embeddings\n",
        "\n",
        "OpenAI’s embedding endpoint caps *each request* at **300 k tokens**. The helper below measures every document, adds them to a running total, and starts a new batch whenever the next doc would tip us over the limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "929c60f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash and run the below if you need to install the tiktoken package\n",
        "#!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3895a4bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_docs(documents, model='text-embedding-3-small', token_limit=300000):\n",
        "    import tiktoken\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    \n",
        "    batches = []\n",
        "    current_tokens = 0\n",
        "    current_docs = []\n",
        "\n",
        "    for document in documents:\n",
        "        tokens = len(encoding.encode(document[\"content\"]))\n",
        "        \n",
        "        if tokens > token_limit:\n",
        "            raise ValueError(f\"Single document exceeds token limit: {tokens} tokens\")\n",
        "\n",
        "        if current_tokens + tokens > token_limit:\n",
        "            batches.append(current_docs)\n",
        "            current_docs = [document]\n",
        "            current_tokens = tokens\n",
        "        else:\n",
        "            current_docs.append(document)\n",
        "            current_tokens += tokens\n",
        "\n",
        "    if current_docs:\n",
        "        batches.append(current_docs)\n",
        "\n",
        "    return batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9796a370",
      "metadata": {},
      "outputs": [],
      "source": [
        "batches = batch_docs(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885b1d07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the below to install chromadb\n",
        "!pip install chromadb\n",
        "!pip install langchain_chroma\n",
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65635ab2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Initialize the embedding model and vector store\n",
        "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "vector_store = Chroma(collection_name='rss_news_feeds', embedding_function=embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bb81c541",
      "metadata": {},
      "outputs": [],
      "source": [
        "for batch in batches:\n",
        "    texts = [doc[\"content\"] for doc in batch]\n",
        "    metadatas = [doc[\"metadata\"] for doc in batch]\n",
        "    vector_store.add_texts(texts, metadatas=metadatas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371c3dae",
      "metadata": {},
      "source": [
        "## 3. Craft the system prompt\n",
        "\n",
        "This prompt underpins our retrieval chain. We must include the \"{context}\" variable in the prompt as this will bring in the relevant news articles based on our query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2a9a6d25",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = (\n",
        "    \"You are an assistant designed to search through news feeds and find the most relevant news based on a prompt. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't have enough information to infer an answer, say that you dont have enough information to answer the question.\"\n",
        "    \"Each document contains the meta data which has the source, news type and pulled date. If you have two stories from different sources that\"\n",
        "    \" appear to be about the same story prioritize the one with the ealier published time.\"\n",
        "    \"Keep the answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7adf4713",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the below line if you need to install yfinance\n",
        "#!pip install yfinance # We will use yfinance to give some more context about portfolio companies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "099749a0",
      "metadata": {},
      "source": [
        "## 4. Create the RAG Chain\n",
        "\n",
        "Here we put all the components together for our RAG workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bec75a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the below to install langchain and openai\n",
        "# !pip install langchain\n",
        "# !pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d2e3bf2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from datetime import datetime\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4.1\")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 20})\n",
        "qa_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, qa_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rag_over_portfolio",
      "metadata": {},
      "source": [
        "## 5. Create material news summaries for each company\n",
        "\n",
        "For each ticker we:\n",
        "1. Pull a short business description from `yfinance` for extra context.  \n",
        "2. Ask the RAG chain which news **published today** might have a material effect on the stock prices.  \n",
        "3. Put each answer in a document for us to aggregate into our portfolio brief later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98482519",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "from typing import List\n",
        "from openai import OpenAI\n",
        "import yfinance as yf\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# 1. Function to summarize news for a given ticker\n",
        "def summarize_ticker(ticker: str) -> str:\n",
        "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "    info  = yf.Ticker(ticker).info\n",
        "    company     = info.get(\"longName\", ticker)\n",
        "    description = info.get(\"longBusinessSummary\", \"\")\n",
        "    query = (\n",
        "        f\"Today's date is {today}. You MUST only reference news published either today or yesterday. News published any other day is unacceptable!\\n\\n\"\n",
        "        f\"Here is a description of {company}: {description}\\n\\n\"\n",
        "        f\"Which news stories published either today or yesterday are most likely to have a material impact on {company}'s stock? If there isnt anything material, say so.\\n\\n\"\n",
        "        \"Include citations. Remember news published any other day is unacceptable!\"\n",
        "    )\n",
        "    result = rag_chain.invoke({\"input\": query})\n",
        "    return f\"{ticker} – {company}:\\n{result}\\n\"\n",
        "\n",
        "# 2. Generate per-ticker summaries\n",
        "tickers = [\"AAPL\",\"MSFT\",\"NVDA\",\"JNJ\",\"UNH\",\"JPM\",\"V\",\"PG\",\"KO\",\"XOM\"]\n",
        "summaries = [summarize_ticker(t) for t in tickers]\n",
        "\n",
        "# 3. Wrap into Documents for summarization chain\n",
        "docs = [\n",
        "    Document(page_content=s, metadata={\"ticker\": s.split(\"–\")[0]})\n",
        "    for s in summaries\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rollup_brief",
      "metadata": {},
      "source": [
        "## 6. Roll-up briefing\n",
        "\n",
        "A second LLM pass condenses the individual blurbs into a single portfolio brief, formatted in Markdown which we can easily convert to html for the email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff2cebf",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def create_portfolio_breif(docs:List[Document],model=\"gpt-4.1\") -> str:\n",
        "    input = f\"\"\"You are a financial analyst and expert Markdown formatter.\n",
        "\n",
        "        Your task is to synthesize the following stock-specific news briefs into a structured, markdown-formatted report. Focus only on **material information** relevant to the portfolio.\n",
        "\n",
        "        **Instructions:**\n",
        "        - Structure the output using **headings** for each portfolio stock (e.g., ## AAPL).\n",
        "        - Under each stock, use **bullet points** for each insight.\n",
        "        - Use your expert financial skill to determine what actually has a high probability to be material and only highlight those stories. Be selective!\n",
        "        - Povide a clear explaination below the news on why the news could materially affect the stock - what are the potential outcomes? \n",
        "        - Add **in-context citations** (e.g., (Source: Bloomberg, June 14)) with links to back up each point.\n",
        "        - Do **not** wrap the output in code blocks.\n",
        "        - Do **not** include commentary or filler—just structured insights.\n",
        "\n",
        "        Here are the news briefs: {str(docs)}\"\"\"\n",
        "    \n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert financial analyst.\"},\n",
        "            {\"role\": \"user\", \"content\": input}\n",
        "        ],\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "# 3. Create and run a map-reduce summarization chain\n",
        "portfolio_brief = create_portfolio_breif(docs)\n",
        "\n",
        "print(portfolio_brief)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae218502",
      "metadata": {},
      "source": [
        "## 7. Deliver the brief\n",
        "\n",
        "If you are comfortable using your Gmail account then proceed to the GMAIL Insturctions Below.\n",
        "\n",
        "We convert the Markdown to HTML and send it via Gmail SMTP.  \n",
        "Make sure you’ve set `GMAIL_EMAIL`, `GMAIL_APP_PASSWORD`, and `TO_EMAIL` in your environment *before* running the next cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5291d3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unhash the bwlow line if you need to install the markdown package\n",
        "#!pip install markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dabb8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import smtplib\n",
        "from email.message import EmailMessage\n",
        "import markdown as md\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables (e.g. from a .env file)\n",
        "load_dotenv()\n",
        "\n",
        "GMAIL_EMAIL        = os.environ['GMAIL_EMAIL']\n",
        "GMAIL_APP_PASSWORD = os.environ['GMAIL_APP_PASSWORD']\n",
        "TO_EMAIL           = os.environ['TO_EMAIL']\n",
        "\n",
        "def send_email(subject: str, body_md: str):\n",
        "    load_dotenv()\n",
        "    user = os.getenv('GMAIL_EMAIL')           # Use your Gmail address\n",
        "    pwd = os.getenv('GMAIL_APP_PASSWORD')     # Use the 16-digit app password\n",
        "    to   = os.getenv('TO_EMAIL')\n",
        "\n",
        "    msg = EmailMessage()\n",
        "    msg['Subject'], msg['From'], msg['To'] = subject, user, to\n",
        "\n",
        "    html = md.markdown(body_md)\n",
        "    msg.set_content('HTML only', subtype='plain')\n",
        "    msg.add_alternative(html, subtype='html')\n",
        "\n",
        "    with smtplib.SMTP('smtp.gmail.com', 587) as s:\n",
        "        s.starttls()\n",
        "        s.login(user, pwd)\n",
        "        s.send_message(msg)\n",
        "\n",
        "    print(f\"✅ Email sent to {to}\")\n",
        "\n",
        "# Example invocation using your portfolio brief\n",
        "send_email(\"Morning Brief\", portfolio_brief)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a01bbde",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Want to Fully Automate This Workflow?\n",
        "\n",
        "If you're interested in automating this daily news briefing—complete with scheduled runs and email delivery via GitHub Actions follow the instructions in the README file [here](https://github.com/CFA-Institute-RPC/The-Automation-Ahead/tree/main/Retrieval%20Augmented%20Generation/Portfolio%20News%20Updater/automation_scripts#readme). It walks you through setting up environment variables, customizing your portfolio, and deploying the automated workflow."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
