{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c37eba2",
   "metadata": {},
   "source": [
    "# Interfacing Proxy Statements – Case Study\n",
    "\n",
    "This notebook walks through a Retrieval-Augmented Generation (RAG) pipeline using proxy statements from Apple, Amazon, and Microsoft to extract executive compensation data and governance details. It accompanies the *Automation Ahead* article on RAG published on the Research and Policy Center (RPC) website. You can view the article [here](placeholder). You can also run the notebook in Google Colab here: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_lj8tDJp9mq_gV0gzSvuMBaPoCJ3pDrm?usp=sharing).\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "By the end of this notebook, you will be familiar with:\n",
    "\n",
    "- **Building a RAG Workflow**  \n",
    "  Learn how to construct a basic Retrieval-Augmented Generation pipeline for extracting structured data from complex financial documents.\n",
    "\n",
    "- **Using Computer Vision for Metadata Extraction**  \n",
    "  See how tools like Mistral’s OCR API can extract structured headers from PDFs, preserving the context needed for effective document chunking and metadata tagging.\n",
    "\n",
    "- **Incorporating Agentic Function Calling**  \n",
    "  Use function calling to integrate external tools (like a calculator) into your workflow, enabling precise outputs and reducing hallucinations for numerical tasks.\n",
    "\n",
    "- **Evaluating Outputs with LLM-as-a-Judge**  \n",
    "  Understand how to scale your evaluation process by using a large language model to judge the accuracy and relevance of other LLM outputs, especially in large datasets.\n",
    "\n",
    "- **Analyzing Executive Compensation Details From Proxy Statements Using AI**  \n",
    "  Gain practical exposure to the nuances of parsing and comparing executive compensation across companies using AI-driven methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fff2bd",
   "metadata": {},
   "source": [
    "# Install Dependencies\n",
    "We must first install all dependencies for the notebook. You will also need a Openai and Mistral AI API key to complete this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346b574",
   "metadata": {},
   "source": [
    "# Mistral AI Account Setup \n",
    "* To get started, create a Mistral account or sign in at [console.mistral.ai](https://console.mistral.ai/).\n",
    "* Then, navigate to \"Workspace\" and \"Billing\" to add your payment information and activate payments on your account.\n",
    "* After that, go to the \"API keys\" page and make a new API key by clicking \"Create new key\". Make sure to copy the API key, save it safely, and do not share it with anyone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c42c07",
   "metadata": {},
   "source": [
    "# OpenAI Account Setup\n",
    "* Create an OpenAI Platform Account – Go to [platform.openai.com](https://platform.openai.com/docs/overview) and follow the prompts to sign up.\n",
    "* Generate Your API Key – After signing in, navigate to the API keys page to create a new secret key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b0f3e5",
   "metadata": {},
   "source": [
    "# Securly Store Your API Keys\n",
    "To securly store your api keys and access them in this notebook create a .env file in the project directory root and save the api keys as:\n",
    "\n",
    "\n",
    "OPENAI_API_KEY='your Openai key'\n",
    "\n",
    "MISTRAL_API_KEY='your Mistral AI key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install -U langchain-community\n",
    "!pip install pypdf\n",
    "!pip install mistralai\n",
    "!pip install python-dotenv\n",
    "!pip install openai\n",
    "!pip install tqdm\n",
    "!pip install langchain_chroma\n",
    "!pip install langchain_openai\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695526b2",
   "metadata": {},
   "source": [
    "## Step 1: Data Ingestion and Parsing Data\n",
    "Load proxy statements from PDF files using LangChain's PyPDFLoader. These pdfs were taken from the SEC edgar website [here](https://www.sec.gov/edgar/search/#). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8c966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Directory where PDFs are stored\n",
    "pdf_folder = \"../Retrieval Augmented Generation/DEF14A-PDFs\"\n",
    "\n",
    "# Get list of PDF files in the folder\n",
    "pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "# Load each PDF into a list of documents\n",
    "documents = []\n",
    "for file_path in pdf_files:\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    documents.extend(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff81cc",
   "metadata": {},
   "source": [
    "# To parse the pdfs we will use Mistral's OCR API.\n",
    "We use Mistral’s Optical Character Recognition (OCR) API because traditional parsing methods often miss key structural elements like headings. The Mistral's OCR API model has been trained to include these headings in its output which give an LLM key structural elements that can be used as metadata that enhance the LLMs ability to answer questions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9193944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Structured Parsing with OCR\n",
    "#Use Mistral’s OCR API to extract structured markdown text from PDFs.\n",
    "\n",
    "from mistralai import Mistral\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "462e7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from mistralai import DocumentURLChunk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parse_pdf(client: object, pdf_path: Path) -> str:\n",
    "    # Upload PDF to Mistral's OCR service\n",
    "    uploaded_file = client.files.upload(\n",
    "        file={\n",
    "            \"file_name\": pdf_path.stem,\n",
    "            \"content\": pdf_path.read_bytes()\n",
    "        },\n",
    "        purpose=\"ocr\"\n",
    "    )\n",
    "    \n",
    "    # Get URL for the uploaded file from Mistral API\n",
    "    signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
    "\n",
    "    # Process PDF with OCR, returning markdown text\n",
    "    pdf_response = client.ocr.process(\n",
    "        document=DocumentURLChunk(document_url=signed_url.url),\n",
    "        model=\"mistral-ocr-latest\"\n",
    "    )\n",
    "\n",
    "    # Extract and return the markdown content\n",
    "    response_dict = json.loads(pdf_response.model_dump_json())\n",
    "    markdown_text = \"\\n\\n\".join(page[\"markdown\"] for page in response_dict[\"pages\"])\n",
    "    \n",
    "    return markdown_text\n",
    "\n",
    "\n",
    "# Batch process multiple PDFs and save results\n",
    "def batch_process_pdfs_to_markdown(client, input_dir: str, output_dir: str) -> dict:\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    markdown_results = {}\n",
    "\n",
    "    pdf_files = list(input_path.glob(\"*.pdf\"))\n",
    "\n",
    "    for pdf in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        try:\n",
    "            md_text = parse_pdf(client, pdf)\n",
    "            markdown_results[pdf.stem] = md_text\n",
    "\n",
    "            # Save each markdown file separately\n",
    "            md_file_path = output_path / f\"{pdf.stem}.md\"\n",
    "            with open(md_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(md_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {pdf.name}: {e}\")\n",
    "\n",
    "    return markdown_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf286c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../Retrieval Augmented Generation/DEF14A-PDFs\"\n",
    "output_dir = \"../Retrieval Augmented Generation/markdown_outputs\"\n",
    "\n",
    "markdown_data = batch_process_pdfs_to_markdown(client, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf817e2",
   "metadata": {},
   "source": [
    "## Step 2: Chunking the Markdown Files\n",
    "Here we use the structured headers created by the Mistral model to chunk documents. This allows us to keep the same semantic structure of the original document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010c4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "def split_markdown_by_headers(pdf_name: str, markdown: str) -> list:\n",
    "    # Define header levels to split on\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\")\n",
    "    ]\n",
    "    \n",
    "    # Initialize the Markdown splitter\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "    \n",
    "    # Split the markdown content based on headers\n",
    "    md_header_splits = markdown_splitter.split_text(markdown)\n",
    "    \n",
    "    # Attach the original file name as metadata for traceability\n",
    "    for split in md_header_splits:\n",
    "        split.metadata['file_name'] = pdf_name\n",
    "    \n",
    "    return md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec70f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_split_markdown_files(input_dir: str) -> list:\n",
    "    input_path = Path(input_dir)\n",
    "    md_files = list(input_path.glob(\"*.md\"))\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for md_file in tqdm(md_files, desc=\"Splitting Markdown Files\"):\n",
    "        with open(md_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            markdown_content = f.read()\n",
    "        \n",
    "        chunks = split_markdown_by_headers(md_file.stem, markdown_content)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_input_dir = \"../Retrieval Augmented Generation/markdown_outputs\"\n",
    "md_header_splits = batch_split_markdown_files(markdown_input_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141c52d",
   "metadata": {},
   "source": [
    "## Step 3: Embedding and Storing Documents in Vector Database\n",
    "We now embed the chunked documents and store them in a vector database for LLM retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e76db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Store document embeddings in a vector database\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=md_header_splits,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=\"vector_db_md_split\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also load the vectorstore from a directory if already done the previous step. Unhash the below if so.\n",
    "\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# vectorstore = Chroma(persist_directory=\"vector_db_md_split\", embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a9f27",
   "metadata": {},
   "source": [
    "## Step 4: Define the Retriever and Setup the Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fec17e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define your prompt template explicitly\n",
    "template = \"\"\"Execute the following query based only on the following context.\n",
    "\n",
    "Query: \\n {query}\n",
    "\n",
    "Context: \\n {context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a retriever from the vectorstore\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 100})\n",
    "\n",
    "def create_rag_prompt(query: str, retriever, template: str) -> str:\n",
    "\n",
    "\t\t# Create the prompt object from the prompt template\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    # Retrieve relevant documents (context) for a given query\n",
    "    context_docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Format context documents into a single string\n",
    "    context_str = str(context_docs)\n",
    "    \n",
    "    # Format the final prompt we will send to the Openai chat completions API with context and query\n",
    "    final_prompt = prompt.format(context=context_str, query=query)\n",
    "\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa847459",
   "metadata": {},
   "source": [
    "## Step 5: Structured Response Generation using OpenAI\n",
    "Here we give the LLM a (pydantic) structured model called ExplainOutputs so that we can easily parse the answer. Doing so provides more control in our RAG system and reduces the likelihood of the model to provide inconsistent structure in its answer. We also ask the LLM to explain its answer before arriving at it, providing more reasoning power to arrive at the correct answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf66530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Define the structured output\n",
    "class ExplainOutputs(BaseModel):\n",
    "\texplanation: str\n",
    "\tanswer: str\n",
    "\n",
    "# Define the OpenAI API client and load environment API keys\n",
    "client = OpenAI()\n",
    "load_dotenv()\n",
    "\n",
    "def run_rag_query(query:str,retriever,structured_response_model,model):\n",
    "    # Create the prompt using the provided query\n",
    "    final_prompt = create_rag_prompt(\n",
    "        query=query,\n",
    "        retriever=retriever,\n",
    "        template=template\n",
    "    )\n",
    "    # Send the structured prompt to OpenAI's ChatCompletion API\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert at interfacing documents and providing responses in a structured format.\"},\n",
    "            {\"role\": \"user\", \"content\": final_prompt}\n",
    "        ],\n",
    "        response_format=structured_response_model,  # Enforces structured output\n",
    "    )\n",
    "    structured_response = completion.choices[0].message.parsed  \n",
    "    return structured_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec45505",
   "metadata": {},
   "source": [
    "# Testing Single Query Responses\n",
    "Here we test the LLMs ability to answer single queries on the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27b6a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Does Apple have a dedicated committee in place to oversee artificial intelligence?\",\n",
    "    \"What is Microsoft’s policy on executives’ derivatives trading?\",\n",
    "    \"List executive board members and independent board members for Amazon to be elected for the next fiscal period\",\n",
    "    \"What was Satya Nadella’s Total Compensation without the inclusion of other compensation? Give it to me as % of Total Compensation to the 4th decimal place.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# We batch run the queries to the RAG system for demonstration purposes\n",
    "def run_batch_rag_queries(questions: List[str], retriever,structured_response_model,model=\"gpt-4o-mini\"):\n",
    "    results = []\n",
    "    for q in questions:\n",
    "        result = run_rag_query(q, retriever, structured_response_model,model=model)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "responses = run_batch_rag_queries(questions, retriever,ExplainOutputs, model=\"gpt-4.1-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f27c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query,response in zip(questions,responses):\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Answer: {response.answer}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78616964",
   "metadata": {},
   "source": [
    "# Testing Function Calling and Agent Integration For RAG Workflow\n",
    "While the last answer was close to the correct one, which was 99.7854%, we can see that the LLM hallucinated the exact digits of the actual answer. This happens because LLMs rely on prior knowledge to answer questions rather than performing fundamental arithmetic. \n",
    "\n",
    "Now we test to see if by giving the LLM access to a \"calculator\" (in this case a python function) if it can answer correctly. This is an example of using function calling agents which provides the LLM a function to excecute when it needs to perform arithmetic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b419a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai-agents # or `uv add openai-agents`, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a4e382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent,function_tool, Runner\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# The next two classes define the data structure of the input and output for the calculator tool\n",
    "class MathExpression(BaseModel):\n",
    "    expression: str = Field(..., description=\"A valid Python mathematical expression, e.g., '(2.5 + 3) * 4 / (1.5 ** 2)'\")\n",
    "\n",
    "class MathExpressionResponse(BaseModel):\n",
    "    result: str = Field(..., description=\"The result of the evaluated expression as a string.\")\n",
    "\n",
    "@function_tool\n",
    "async def calculator_tool(data: MathExpression) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates a mathematical expression using Python's eval() and returns the result as a string.\n",
    "\n",
    "    Args:\n",
    "        data (MathExpression): An object containing a math expression to evaluate (e.g., '(2 + 3) * 4').\n",
    "\n",
    "    Returns:\n",
    "        str: The result of the evaluated expression, formatted as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Optional: restrict available built-ins for safety\n",
    "        result = eval(data.expression, {\"__builtins__\": None}, {})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec707e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the agents that will use the calculator tool and the expression agent\n",
    "expression_agent = Agent(\n",
    "    name=\"Math Expression Agent\",\n",
    "    instructions=\"You are a math assistant specialized in creating mathematical expressions that work with python's eval function.\",\n",
    "    output_type=MathExpression,\n",
    "    model='gpt-4.1-mini'\n",
    ")\n",
    "\n",
    "expression_agent_tool = expression_agent.as_tool(\n",
    "    tool_name=\"expression_agent_tool\",\n",
    "    tool_description=\"Create a mathematical expression that will work with pythons eval function from the input.\"\n",
    ")\n",
    "\n",
    "calculator_agent = Agent(\n",
    "    name=\"Financial Math Agent\",\n",
    "    instructions=\"You are a math assistant specialized in financial analysis. Ceate a mathematical expression and use the calculator_tool for the precise calculations.\",\n",
    "    output_type=MathExpressionResponse,\n",
    "    tools=[calculator_tool],\n",
    "    model='gpt-4.1-mini'\n",
    ")\n",
    "\n",
    "calculator_agent_tool = calculator_agent.as_tool(\n",
    "    tool_name=\"calculator_agent_tool\",\n",
    "    tool_description=\"Evaluate mathematical expressions using Python for financial questions.\"\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"triage_agent\",\n",
    "    instructions=\"You are a triage agent specialized in financial analysis. You will triage the question to determine which agent to pass the request off to.\",\n",
    "    tools=[calculator_agent_tool, expression_agent_tool],\n",
    "    model='gpt-4.1-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f7d278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Agent, what is the total compensation of Satya Nadella without the inclusion of other compensation? Give it to me as % of Total Compensation to the 4th decimal place.\"\n",
    "prompt = create_rag_prompt(\n",
    "        query=query,\n",
    "        retriever=retriever,\n",
    "        template=template\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b7f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(triage_agent, prompt)\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a81ac",
   "metadata": {},
   "source": [
    "# Testing Realized Compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ff9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is Tim Cook’s realized long-term Incentives for the fiscal year 2024 based on shares vested?'\n",
    "response = run_rag_query(question, retriever, structured_response_model=ExplainOutputs, model=\"gpt-4.1-mini\",)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7ee7e",
   "metadata": {},
   "source": [
    "# Automated Multi-Company Multi-Variable Extraction\n",
    "We now test the limitations of the RAG workflows by giving it a much more complex task which included extracting multiple variables from multiple companies and providing the answer in a structured table for analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed2b2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['Apple', 'Amazon', 'Microsoft']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898083f",
   "metadata": {},
   "source": [
    "# Create the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a685f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"You are an expert financial analysis assistant specializing in extracting detailed executive compensation data from proxy statements. Your task is to extract the following fields from the provided proxy statements of {\",\".join(companies)} and output the information in the given structure. For any missing or not applicable information, use “N/A”.\n",
    "\n",
    "Extract these fields for each company:\n",
    "1. Company Name: Full legal name of the company.\n",
    "2. Company CEO: Name of the Chief Executive Officer.\n",
    "3. Coverage Period: The fiscal period covered by the proxy (e.g., \"Fiscal Year 2024\" or \"Fiscal Year Ended June 30, 2024\").\n",
    "4. Total Target LTI (Full Grant Amount): The total target long-term incentive amount, including all equity components.\n",
    "5. LTI Package Grant Date: The date when the LTI grant was awarded.\n",
    "6. Annual LTI Grant?: Indicate “Yes” if equity awards are granted annually, or “No” if not.\n",
    "7. Time-Based RSU Vesting Schedule: Details on vesting for time-based equity (e.g., \"Three equal annual installments starting April 1, 2026\" or quarterly vesting over a specified period). If not applicable, indicate “N/A”.\n",
    "8. Performance-Based RSU Vesting Schedule: Details on vesting for performance-based equity, including the performance period and conditions (e.g., \"Vests on October 1, 2026 based on performance from October 1, 2023 to September 30, 2026\"). If not applicable, indicate “N/A”.\n",
    "9. Compensation Governance Arrangements: Information on the oversight mechanisms (e.g., independent compensation committees, clawback provisions, and stock ownership policies).\n",
    "10. CEO Pay Alignment Mechanisms: How the compensation is structured to align CEO pay with long-term shareholder value.\n",
    "11. Performance Metrics Used (Detailed): Provide specifics of the performance metrics applied, including the measurement period and targets (e.g., \"Relative TSR compared to the S&P 500 over a 3-year period (October 2023 – October 2026), payout ranges from 0%–200%\" or details on revenue growth targets).\n",
    "12. Realized Base Salary: Actual base salary paid in the period.\n",
    "13. Realized STIs: Actual short-term incentives (bonuses) paid.\n",
    "14. Realized Long-Term Awards: Value of equity awards that have vested in the period.\n",
    "15. Realized Other Compensation: Additional benefits (e.g., security costs, deferred compensation, change-in-control benefits).\n",
    "16. Realized Total Compensation: The sum of all compensation elements actually received in the period.\n",
    "\n",
    "Output your findings in a JSON format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b320ef",
   "metadata": {},
   "source": [
    "# Create the Stuctured Output Model\n",
    "Here we create the structured model for the output to be converted into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef503894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "from datetime import date\n",
    "\n",
    "class ExecutiveCompensation(BaseModel):\n",
    "    company_name: str\n",
    "    company_ceo: str\n",
    "    coverage_period: str\n",
    "    total_target_lti: Optional[float] = Field(..., description=\"Target LTI in USD\")\n",
    "    lti_grant_date: Optional[str]\n",
    "    annual_lti_grant: Optional[bool]\n",
    "    time_based_rsu_vesting_schedule: Optional[str]\n",
    "    performance_based_rsu_vesting_schedule: Optional[str]\n",
    "    compensation_governance_arrangements: Optional[str]\n",
    "    ceo_pay_alignment_mechanisms: Optional[str]\n",
    "    performance_metrics_used: Optional[str]\n",
    "    realized_base_salary: Optional[float]\n",
    "    realized_stis: Optional[float]\n",
    "    realized_long_term_awards: Optional[float]\n",
    "    realized_other_compensation: Optional[float]\n",
    "    realized_total_compensation: Optional[float]\n",
    "\n",
    "class ExecutiveCompensationReport(BaseModel):\n",
    "    companies: List[ExecutiveCompensation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6784fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "executive_compensation_report = run_rag_query(query, retriever, structured_response_model=ExecutiveCompensationReport, model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a534153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "executive_compensation_report_json = executive_compensation_report.model_dump()\n",
    "executive_compensation_report_db = pd.DataFrame(executive_compensation_report_json['companies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "executive_compensation_report_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5b474",
   "metadata": {},
   "source": [
    "# Evaluating LLM Responses Using an LLM Judge\n",
    "We now evaluate the response using an LLM as a Judge. Here we compare the groundtruth labels which were completed manually by a human, against the LLM outputed answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "28efdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ground_truth_db = pd.read_excel('../Retrieval Augmented Generation/ground_truth_table.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a2cffe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fist, we need to convert both dataframes from wide to long format so we can merge both dataframes and compare them horizontally\n",
    "ground_truth_db_melted = ground_truth_db.melt(id_vars=['company_name'], var_name='field', value_name='ground_truth_db_value')\n",
    "executive_compensation_report_melted = executive_compensation_report_db.melt(id_vars=['company_name'], var_name='field', value_name='exectuive_compensation_report_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d270aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can merge the two dataframes on company_name and field\n",
    "combined_db = ground_truth_db_melted.merge(executive_compensation_report_melted, on=['company_name', 'field'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f97f4",
   "metadata": {},
   "source": [
    "# Create the Judge\n",
    "Here we create the output model and the LLM judge used for evaluating the answers against the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "718fc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the updated structured output model with explanation\n",
    "class EvaluationOutput(BaseModel):\n",
    "    field: str\n",
    "    model_answer: str\n",
    "    ground_truth: str\n",
    "    score: float\n",
    "    explanation: str\n",
    "\n",
    "# Define the function to run the LLM Judge query\n",
    "def run_llm_judge_query(query: str, field:str, model: str, ground_truth: dict, model_generated: dict, structured_response_model: EvaluationOutput):\n",
    "    # Create the prompt for the LLM Judge to compare answers\n",
    "    prompt = f\"\"\"\n",
    "    Evaluate the accuracy of the model's response for the following fields by comparing it to the ground truth. \n",
    "    For each field, provide a score between 0 and 1:\n",
    "    0 means completely incorrect or missing.\n",
    "    1 means fully correct and accurate.\n",
    "    \n",
    "    Additionally, provide an explanation of why the score was given.\n",
    "    \n",
    "    Query: {query}\n",
    "\n",
    "    Field: {field}\n",
    "    \n",
    "    Model Answer: {model_generated}\n",
    "    Ground Truth: {ground_truth}\n",
    "    \n",
    "    Compare the two and assign a score for each field, along with an explanation for the score.\n",
    "    Return the score and explanation in structured JSON format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the structured prompt to OpenAI's ChatCompletion API\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert evaluator comparing model outputs to ground truth.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=structured_response_model,  # Enforces structured output\n",
    "    )\n",
    "    structured_response = completion.choices[0].message.parsed\n",
    "\n",
    "    return structured_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "507a9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_results = []\n",
    "\n",
    "for index, row in combined_db.iterrows():\n",
    "    field = row['field']\n",
    "    model_answer = row['exectuive_compensation_report_value']\n",
    "    ground_truth = row['ground_truth_db_value']\n",
    "\n",
    "    # Run the LLM Judge query\n",
    "    evaluation_result = run_llm_judge_query(\n",
    "        query=query,\n",
    "        field=field,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        ground_truth=ground_truth,\n",
    "        model_generated=model_answer,\n",
    "        structured_response_model=EvaluationOutput\n",
    "    )\n",
    "    evaluations_results.append(evaluation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "514b13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a new column for the LLM score on our combined dataframe\n",
    "combined_db['executive_compensation_score'] = [evaluations_result.score for evaluations_result in evaluations_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see the results\n",
    "combined_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecfc410",
   "metadata": {},
   "source": [
    "# Plot the Scores as a Heatmap\n",
    "Lastly, we can visualize our LLM accuracy using a heatmap for each variable and company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8193e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "98a7cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_accuracy_heatmap(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Dictionary to rename fields for better readability\n",
    "    field_rename_map = {\n",
    "        \"company_ceo\": \"CEO\",\n",
    "        \"coverage_period\": \"Coverage Period\",\n",
    "        \"total_target_lti\": \"Total Target LTI\",\n",
    "        \"lti_grant_date\": \"LTI Grant Date\",\n",
    "        \"annual_lti_grant\": \"Annual LTI Grant?\",\n",
    "        \"time_based_rsu_vesting_schedule\": \"Time-Based RSU Vesting\",\n",
    "        \"performance_based_rsu_vesting_schedule\": \"Performance-Based RSU Vesting\",\n",
    "        \"compensation_governance_arrangements\": \"Governance Arrangements\",\n",
    "        \"ceo_pay_alignment_mechanisms\": \"CEO Pay Alignment\",\n",
    "        \"performance_metrics_used\": \"Performance Metrics\",\n",
    "        \"realized_base_salary\": \"Realized Base Salary\",\n",
    "        \"realized_stis\": \"Realized STIs\",\n",
    "        \"realized_long_term_awards\": \"Realized LT Awards\",\n",
    "        \"realized_other_compensation\": \"Realized Other Compensation\",\n",
    "        \"realized_total_compensation\": \"Realized Total Compensation\"\n",
    "    }\n",
    "\n",
    "    # Rename the 'field' column according to the mapping\n",
    "    df['field'] = df['field'].map(field_rename_map)\n",
    "\n",
    "    # Define the correct order of fields\n",
    "    correct_field_order = [\n",
    "        \"CEO\", \"Coverage Period\", \"Total Target LTI\", \"LTI Grant Date\", \"Annual LTI Grant?\",\n",
    "        \"Time-Based RSU Vesting\", \"Performance-Based RSU Vesting\", \"Governance Arrangements\",\n",
    "        \"CEO Pay Alignment\", \"Performance Metrics\", \"Realized Base Salary\", \"Realized STIs\",\n",
    "        \"Realized LT Awards\", \"Realized Other Compensation\", \"Realized Total Compensation\"\n",
    "    ]\n",
    "\n",
    "    # Ensure the 'field' column has the correct order\n",
    "    df['field'] = pd.Categorical(df['field'], categories=correct_field_order, ordered=True)\n",
    "    \n",
    "    # Pivot the DataFrame to get company names as rows and fields as columns\n",
    "    pivot_df = df.pivot(index=\"company_name\", columns=\"field\", values=\"executive_compensation_score\")\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(pivot_df, annot=True, cmap=\"RdYlGn\", linewidths=0.5, cbar_kws={'label': 'Accuracy Score'}, vmin=0, vmax=1)\n",
    "    plt.title(\"LLM Accuracy Heatmap for Executive Compensation Extraction\")\n",
    "    plt.xlabel(\"Variable\")\n",
    "    plt.ylabel(\"Company Name\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_accuracy_heatmap(combined_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf727b0a",
   "metadata": {},
   "source": [
    "### Note scores will depend on the models used. The original article on the Research and Policy Center website used gpt4o-mini. Your results may differ. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
