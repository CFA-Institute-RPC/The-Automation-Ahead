{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation Evaluation\n",
    "\n",
    "This tutorial demonstrates how you can use the [automation framework](https://rpc.cfainstitute.org/research/the-automation-ahead-content-series/introduction) to quickly assess the automation potential of an investment task. This notebook guides you through a programmatic approach to running this process with the following components:\n",
    "\n",
    "1. **Create an Interactive Form**  \n",
    "   Use `ipywidgets` in Jupyter Notebook to collect user inputs, such as task descriptions and ratings for various attributes related to automation suitability.\n",
    "\n",
    "2. **Format Inputs into a Prompt**  \n",
    "   Leverage LangChain's `PromptTemplate` to structure the collected inputs into a well-organized format optimized for GPT processing.\n",
    "\n",
    "3. **Integrate OpenAI Chat Completion**  \n",
    "   Use the OpenAI API to send the formatted prompt to a GPT model and retrieve a detailed evaluation or recommendation.\n",
    "\n",
    "4. **Save GPT Outputs**  \n",
    "   Save the model's response to a markdown file for documentation, sharing, or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (1.57.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/brianpisaneschi/OneDrive - CFA Institute/Coding/GitHub/The-Automation-Ahead/conda-env/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Chat Completion\n",
    "This section integrates the OpenAI API to send the formatted gpt prompt and retrieve the GPT response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "#create a .env file with your openai_api_key\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "# Or set your OpenAI API key with the below\n",
    "# openai_api_key = \"your_openai_api_key\"\n",
    "# client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "\n",
    "def run_gpt_chat_completion(input_text,model='gpt-4o'):\n",
    "    \"\"\"\n",
    "    Send the input text to OpenAI GPT and get a chat completion.\n",
    "    \n",
    "    Args:\n",
    "        input_text (str): The input prompt to send to the GPT model.\n",
    "    \n",
    "    Returns:\n",
    "        str: The response from the GPT model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an automation framework evaluator.\"},\n",
    "                {\"role\": \"user\", \"content\": input_text}\n",
    "            ],\n",
    "            max_tokens=1500,\n",
    "            temperature=0  # Lower temperature for deterministic outputs\n",
    "        )\n",
    "        return response.choices[0].message\n",
    "    except Exception as e:\n",
    "        print(f\"Error with OpenAI API: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form for Input\n",
    "Here, we create an interactive form using `ipywidgets` to collect user inputs, such as task description and ratings for automation suitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill out the following form to assess automation suitability:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18889848ccb34ff7ad6559940f5e8604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Task Description:', layout=Layout(height='100px', width='600px'), placeholder=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "<style>\n",
       "    table {\n",
       "        border-collapse: collapse;\n",
       "        width: 100%;\n",
       "    }\n",
       "    th, td {\n",
       "        border: 1px solid #ddd;\n",
       "        padding: 8px;\n",
       "        text-align: left;\n",
       "    }\n",
       "    th {\n",
       "        background-color: #f2f2f2;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th>Attribute</th>\n",
       "            <th>Rating (1-5)</th>\n",
       "        </tr>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c382dbb810534f5ca4048960aa98ba86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Task Complexity', layout=Layout(width='200px')), IntSlider(value=3, description='I…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5350a88f35e42a7a42e9852671584be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Output Objectivity', layout=Layout(width='200px')), IntSlider(value=3, description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065b7acb499741a896be9891df3e6084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Data Structure', layout=Layout(width='200px')), IntSlider(value=3, description='Is…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e521f54c40446994e0f8b99768d591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Risk Level', layout=Layout(width='200px')), IntSlider(value=3, description='What i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10ef192deb44edc8e422a12dfe65dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Human Oversight Requirement', layout=Layout(width='200px')), IntSlider(value=3, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd29b6d902c9428db2095b2ca5af4b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Impact on Efficiency', layout=Layout(width='200px')), IntSlider(value=3, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5539cd1e898d476c97a880142881e012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fde67215fc4d90a4c72d90881d5156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Styling for the table\n",
    "table_style = \"\"\"\n",
    "<style>\n",
    "    table {\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "    }\n",
    "    th, td {\n",
    "        border: 1px solid #ddd;\n",
    "        padding: 8px;\n",
    "        text-align: left;\n",
    "    }\n",
    "    th {\n",
    "        background-color: #f2f2f2;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# Input for task description\n",
    "task_description = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Describe the task for automation\",\n",
    "    description=\"Task Description:\",\n",
    "    layout=widgets.Layout(width=\"600px\", height=\"100px\"),\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "# Define the attributes and their descriptions\n",
    "attributes = [\n",
    "    (\"Task Complexity\", \"Is the task repetitive (1) or highly variable (5)?\"),\n",
    "    (\"Output Objectivity\", \"Are the outputs objective (1) or subjective (5)?\"),\n",
    "    (\"Data Structure\", \"Is the data structured (1) or unstructured (5)?\"),\n",
    "    (\"Risk Level\", \"What is the potential risk of automation failure? Low (1), High (5)\"),\n",
    "    (\"Human Oversight Requirement\", \"Does the task need human validation or sign-off? No (1), Yes (5)\"),\n",
    "    (\"Impact on Efficiency\", \"How much time or effort can automation save? Little (1), a lot (5)\")\n",
    "]\n",
    "\n",
    "# Create widgets for each row\n",
    "rows = []\n",
    "for attribute, description in attributes:\n",
    "    slider = widgets.IntSlider(\n",
    "        min=1, max=5, value=3, \n",
    "        description=description, \n",
    "        style={'description_width': '400px'}, \n",
    "        layout=widgets.Layout(width=\"600px\")\n",
    "    )\n",
    "    rows.append((attribute, slider))\n",
    "\n",
    "# Global variable to store inputs\n",
    "collected_inputs = {}\n",
    "\n",
    "# Function to display the form\n",
    "def display_table(rows):\n",
    "    # Render table headers\n",
    "    header_html = f\"\"\"\n",
    "    {table_style}\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Attribute</th>\n",
    "            <th>Rating (1-5)</th>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "    display(HTML(header_html))\n",
    "\n",
    "    # Display each row\n",
    "    for attribute, slider in rows:\n",
    "        row_box = widgets.HBox([\n",
    "            widgets.Label(attribute, layout=widgets.Layout(width=\"200px\")),\n",
    "            slider\n",
    "        ])\n",
    "        display(row_box)\n",
    "\n",
    "# Output area to capture form submission\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to collect inputs and update the global variable\n",
    "def handle_submit(b):\n",
    "    global collected_inputs\n",
    "    with output:\n",
    "        output.clear_output()  # Clear previous output\n",
    "        # Collect task description\n",
    "        task_desc = task_description.value\n",
    "\n",
    "        # Collect scorecard values\n",
    "        scorecard_values = {}\n",
    "        for attribute, slider in rows:\n",
    "            scorecard_values[attribute] = slider.value\n",
    "\n",
    "        # Store the collected inputs in the global dictionary\n",
    "        collected_inputs = {\n",
    "            \"task_description\": task_desc,\n",
    "            \"scorecard\": scorecard_values\n",
    "        }\n",
    "\n",
    "        # Display the collected inputs for debugging or confirmation\n",
    "        print(\"=== Collected Inputs ===\")\n",
    "        print(collected_inputs)\n",
    "\n",
    "# Attach the function to the Submit button\n",
    "submit_button = widgets.Button(\n",
    "    description=\"Submit\",\n",
    "    button_style=\"success\"\n",
    ")\n",
    "submit_button.on_click(handle_submit)\n",
    "\n",
    "# Display the form\n",
    "print(\"Fill out the following form to assess automation suitability:\")\n",
    "display(task_description)\n",
    "display_table(rows)\n",
    "display(submit_button, output)\n",
    "\n",
    "# Access the collected inputs after submission\n",
    "def get_collected_inputs():\n",
    "    return collected_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "We define a structured prompt template using LangChain's `PromptTemplate` to format the user inputs into a well-organized structure for GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "Using this framework developed for GenAI task automation:\n",
    "| Attribute | GenAI Automation | \n",
    "|-----------|------------------| \n",
    "| Data Type | Unstructured data (e.g., earnings transcripts, market news) | \n",
    "| Task Variability | Repetitive and Variable tasks (e.g., customizing strategies, client communications) | \n",
    "| Input Objectivity | Handles subjective, ambiguous inputs (e.g., free-form text) | \n",
    "| Output Objectivity | Stochastic, probabilistic outputs (e.g., personalized reports) | \n",
    "| Scalability | Scalable but with potential high computational cost | \n",
    "\n",
    "I would like to assess the following task: \n",
    "{task_description}\n",
    "\n",
    "I have rated the task across various attributes using the following scorecard:\n",
    "Scorecard:\n",
    "- Task Complexity: {task_complexity} (1: repetitive, 5: highly variable)\n",
    "- Output Objectivity: {output_objectivity} (1: objective, 5: subjective)\n",
    "- Data Structure: {data_structure} (1: structured, 5: unstructured)\n",
    "- Risk Level: {risk_level} (1: low risk, 5: high risk)\n",
    "- Human Oversight Requirement: {human_oversight} (1: no, 5: yes)\n",
    "- Impact on Efficiency: {impact_efficiency} (1: little impact, 5: significant impact)\n",
    "\n",
    "Based on this scorecard, please provide a score for the overall GenAI automation suitability, \n",
    "a hybrid approach fit score, and the recommended hybrid approach, if appropriate, for automating \n",
    "this task, specifying where traditional automation, GenAI, and human intervention would be most effective.\n",
    " Consider the task’s variability, output objectivity, and potential risks in your recommendation\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate instance\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"task_description\",\n",
    "        \"task_complexity\",\n",
    "        \"output_objectivity\",\n",
    "        \"data_structure\",\n",
    "        \"risk_level\",\n",
    "        \"human_oversight\",\n",
    "        \"impact_efficiency\"\n",
    "    ],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted GPT Prompt:\n",
      "\n",
      "\n",
      "Using this framework developed for GenAI task automation:\n",
      "| Attribute | GenAI Automation | \n",
      "|-----------|------------------| \n",
      "| Data Type | Unstructured data (e.g., earnings transcripts, market news) | \n",
      "| Task Variability | Repetitive and Variable tasks (e.g., customizing strategies, client communications) | \n",
      "| Input Objectivity | Handles subjective, ambiguous inputs (e.g., free-form text) | \n",
      "| Output Objectivity | Stochastic, probabilistic outputs (e.g., personalized reports) | \n",
      "| Scalability | Scalable but with potential high computational cost | \n",
      "\n",
      "I would like to assess the following task: \n",
      "The task is to conduct performance attribution for a client’s overall portfolio, which includes multiple mutual funds. This involves breaking down the portfolio's performance to identify the contributions from stock selection and asset allocation. Additionally, it provides context by analyzing macroeconomic and sector events that may have influenced performance over the quarter.\n",
      "\n",
      "I have rated the task across various attributes using the following scorecard:\n",
      "Scorecard:\n",
      "- Task Complexity: 4 (1: repetitive, 5: highly variable)\n",
      "- Output Objectivity: 4 (1: objective, 5: subjective)\n",
      "- Data Structure: 2 (1: structured, 5: unstructured)\n",
      "- Risk Level: 2 (1: low risk, 5: high risk)\n",
      "- Human Oversight Requirement: 1 (1: no, 5: yes)\n",
      "- Impact on Efficiency: 4 (1: little impact, 5: significant impact)\n",
      "\n",
      "Based on this scorecard, please provide a score for the overall GenAI automation suitability, \n",
      "a hybrid approach fit score, and the recommended hybrid approach, if appropriate, for automating \n",
      "this task, specifying where traditional automation, GenAI, and human intervention would be most effective.\n",
      " Consider the task’s variability, output objectivity, and potential risks in your recommendation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the collected inputs from the form\n",
    "inputs = get_collected_inputs()\n",
    "\n",
    "# Format the prompt using the collected inputs\n",
    "formatted_prompt = prompt_template.format(\n",
    "    task_description=inputs[\"task_description\"],\n",
    "    task_complexity=inputs[\"scorecard\"][\"Task Complexity\"],\n",
    "    output_objectivity=inputs[\"scorecard\"][\"Output Objectivity\"],\n",
    "    data_structure=inputs[\"scorecard\"][\"Data Structure\"],\n",
    "    risk_level=inputs[\"scorecard\"][\"Risk Level\"],\n",
    "    human_oversight=inputs[\"scorecard\"][\"Human Oversight Requirement\"],\n",
    "    impact_efficiency=inputs[\"scorecard\"][\"Impact on Efficiency\"]\n",
    ")\n",
    "\n",
    "print(\"Formatted GPT Prompt:\\n\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Output\n",
    "Finally, we run the formatted input and display the GPT response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# GPT Response\n",
       "\n",
       "To assess the suitability of GenAI automation for the task of conducting performance attribution for a client's portfolio, we need to consider the task's complexity, output objectivity, data structure, risk level, human oversight requirement, and impact on efficiency. Here's a breakdown of the scores and recommendations:\n",
       "\n",
       "### Overall GenAI Automation Suitability Score\n",
       "\n",
       "1. **Task Complexity (4/5):** The task is highly variable, involving analysis and interpretation of financial data, which suits GenAI's ability to handle complex, variable tasks.\n",
       "   \n",
       "2. **Output Objectivity (4/5):** The task requires subjective analysis and interpretation, aligning well with GenAI's strength in generating probabilistic outputs.\n",
       "\n",
       "3. **Data Structure (2/5):** The task involves semi-structured data (financial data) and unstructured data (macroeconomic analysis), which GenAI can handle but may require additional preprocessing.\n",
       "\n",
       "4. **Risk Level (2/5):** The task has a moderate risk level, as financial analysis impacts decision-making. GenAI can assist but should be used cautiously.\n",
       "\n",
       "5. **Human Oversight Requirement (1/5):** Minimal human oversight is required, suggesting that GenAI can automate significant portions of the task.\n",
       "\n",
       "6. **Impact on Efficiency (4/5):** Automating this task with GenAI can significantly improve efficiency by quickly analyzing large datasets and generating insights.\n",
       "\n",
       "**Overall GenAI Suitability Score:** 3.5/5\n",
       "\n",
       "### Hybrid Approach Fit Score\n",
       "\n",
       "Given the complexity and variability of the task, a hybrid approach is likely more suitable. This approach combines traditional automation, GenAI, and human intervention to balance efficiency and accuracy.\n",
       "\n",
       "**Hybrid Approach Fit Score:** 4/5\n",
       "\n",
       "### Recommended Hybrid Approach\n",
       "\n",
       "1. **Traditional Automation:**\n",
       "   - **Data Collection and Preprocessing:** Use traditional automation to gather and preprocess structured data from financial databases and reports. This includes extracting performance data of mutual funds and portfolio allocations.\n",
       "\n",
       "2. **GenAI:**\n",
       "   - **Performance Analysis and Attribution:** Utilize GenAI to analyze the performance data, identify contributions from stock selection and asset allocation, and generate insights on macroeconomic and sector events. GenAI can handle the subjective interpretation of how these factors influenced performance.\n",
       "\n",
       "3. **Human Intervention:**\n",
       "   - **Validation and Contextual Analysis:** Financial analysts should review GenAI-generated reports to ensure accuracy and provide additional context. Human expertise is crucial for interpreting nuanced market conditions and making final recommendations.\n",
       "\n",
       "By leveraging traditional automation for data handling, GenAI for complex analysis, and human oversight for validation, this hybrid approach maximizes efficiency while minimizing risks associated with subjective financial analysis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "response = run_gpt_chat_completion(formatted_prompt)\n",
    "\n",
    "markdown_response = f'# GPT Response\\n\\n{response.content}'\n",
    "\n",
    "display(Markdown(markdown_response))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
